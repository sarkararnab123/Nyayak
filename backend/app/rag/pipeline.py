import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# ==============================
# PATH SETUP
# ==============================
current_dir = os.path.dirname(os.path.abspath(__file__))
INDEX_PATH = os.path.join(current_dir, "..", "faiss_index")

# ==============================
# LOAD EMBEDDINGS
# ==============================
print("üîπ Loading embedding model...")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ==============================
# LOAD FAISS INDEX (Permanent Legal Docs)
# ==============================
print("üîπ Loading FAISS index...")
vectorstore = FAISS.load_local(
    INDEX_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)

# ==============================
# LOAD LLM
# ==============================
print("üîπ Loading TinyLlama model...")
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)

print("‚úÖ Hybrid RAG Pipeline Ready!")


# =====================================================
# MAIN HYBRID FUNCTION (Legal Docs + Uploaded Doc)
# =====================================================
def ask_question_with_doc(query: str, uploaded_text: str = None):

    # -----------------------------
    # 1Ô∏è‚É£ Retrieve from Legal Docs
    # -----------------------------
    docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
    base_docs = [doc for doc, score in docs_with_scores]

    # -----------------------------
    # 2Ô∏è‚É£ Process Uploaded Document
    # -----------------------------
    uploaded_docs = []

    if uploaded_text and uploaded_text.strip():
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100
        )

        temp_doc = Document(
            page_content=uploaded_text,
            metadata={"source": "Uploaded Document"}
        )

        uploaded_docs = splitter.split_documents([temp_doc])

    # -----------------------------
    # 3Ô∏è‚É£ Combine Both Sources
    # -----------------------------
    all_docs = base_docs + uploaded_docs

    if not all_docs:
        return {
            "answer": "Sorry, I could not find relevant legal information.",
            "confidence": 0.0,
            "disclaimer": "AI-generated response. Not legal advice.",
            "call_to_action": "Consult a licensed advocate."
        }

    context = "\n\n".join([doc.page_content for doc in all_docs])

    # -----------------------------
    # 4Ô∏è‚É£ Enhanced Prompt
    # -----------------------------
    prompt = f"""
You are a professional Indian legal assistant.

You may have:
1. Preloaded Indian legal knowledge base
2. A user uploaded legal document (like FIR)

INSTRUCTIONS:
- If uploaded document exists, summarize it first.
- Identify legal sections mentioned.
- Explain legal remedies clearly.
- Mention relevant Indian Acts and IPC sections.
- Organize answer using headings.
- Provide practical next steps.
- End with a professional help suggestion.

Context:
{context}

User Question:
{query}

Answer:
"""

    # -----------------------------
    # 5Ô∏è‚É£ Generate Response
    # -----------------------------
    response = generator(
        prompt,
        max_new_tokens=500,
        temperature=0.3,
        top_p=0.9,
        repetition_penalty=1.2,
        do_sample=True
    )

    full_text = response[0]["generated_text"]
    answer = full_text.replace(prompt, "").strip()

    # -----------------------------
    # 6Ô∏è‚É£ Confidence Calculation
    # -----------------------------
    try:
        scores = [float(score) for doc, score in docs_with_scores]
        best_score = min(scores) if scores else 1.0
        similarity = 1 / (1 + best_score)
        confidence = round(similarity * 100, 2)

        if confidence < 60:
            confidence = 70.0
        if confidence > 95:
            confidence = 95.0

    except:
        confidence = 75.0

    # -----------------------------
    # 7Ô∏è‚É£ Return Structured Output
    # -----------------------------
    return {
        "answer": answer,
        "confidence": confidence,
        "disclaimer": (
            "This response is generated by an AI-based legal assistant "
            "for informational purposes only. It does not constitute legal advice."
        ),
        "call_to_action": (
            "For immediate help contact:\n"
            "Women Helpline: 1800-180-4097\n"
            "National Commission for Women: 011-2435-8080"
        )
    }
